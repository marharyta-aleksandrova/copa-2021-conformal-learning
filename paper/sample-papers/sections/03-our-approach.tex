\section{Combination of \textit{inverse probability} and \textit{margin} nonconformity functions}
\label{sec:our-approach}

As was shown by \cite{johansson2017model}, the usage of \textit{inverse probability}
nonconformity function results in less number of predicted class labels on average 
(lower $avgC$), and \textit{margin} results in a  larger fraction of singleton 
predictions (higher $oneC$). In this section, we propose an approach to combine
these properties of the two nonconformity functions. The validity of this method
is studied empirically in \cref{sec:experiments:validity} and its efficiency is 
demonstrated in \cref{sec:experiments:efficiency}.

It is desirable to have more singleton predictions. However, if a singleton prediction
does not contain the true label, then the metric $oneC$ not only loses its
value but also becomes misleading. In \cref{sec:experiments:eff-oneC} we
demonstrate that for some datasets only a half of singleton predictions contain 
the true label. Hence, in our proposed method we decide to take the 
results produced by \textit{inverse probability} nonconformity function as 
a baseline, and then extend them with some singleton predictions resulting 
from the usage of \textit{margin}. 

The proposed procedure is the following.
\textbf{First}, we construct conformal predictors using both nonconformity 
functions separately\footnote{
See \cite{vovk2005algorithmic,shafer2008tutorial,johansson2017model} for
explanation of how conformal predictors are constructed.
}. For the conformal predictor based on \textit{inverse 
probability}, we use the value of $\epsilon$ specified by the user as the
significance level. 
For the conformal predictor based on \textit{margin}
we set the significance level equal to $\epsilon / 2$. This is done to compensate for possible erroneous singleton
predictions produced by \textit{margin} nonconformity function and to achieve the
required level or empirical error rate. \textbf{Second}, for every instance in
the testing or production dataset, we analyze the predictions generated by both 
conformal classifiers. If the conformal classifier based on \textit{margin} outputs 
a singleton and the other conformal classifier not, then the prediction is
taken from the first model. Otherwise, the output of the conformal classifier 
based on \textit{inverse probability} is used.
Such a combination will perform in the worst case the same as the conformal
predictor based on \textit{inverse probability}. Otherwise, the values of
$oneC$ and/or $avgC$ will be improved, as some non-singleton predictions will
be replaced with singletons. Thereby, in case the validity is preserved, this
combination can be considered as an improved version of the 
\textit{inverse probability} nonconformity function.

In the rest of the paper, we will use \textit{M} and \textit{IP} to refer to the 
conformal classifiers based on \textit{margin} and \textit{inverse probability}
respectively. \textit{IP\_M} will be used to refer to the combination explained
above.
%referred to as a nonconformity function, although technically it is not.
For simplicity, sometimes \textit{IP\_M} will be
referred to as a nonconformity function, although technically it is not.

