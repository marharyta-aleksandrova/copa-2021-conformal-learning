{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts2 import *\n",
    "from scripts import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'analysis-results/experimental-results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_sign = 'analysis-results/significance/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing a dataset by index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_name = data_order[i]\n",
    "print(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets_load[dataset_name]\n",
    "n_classes = len(data['target_names'])\n",
    "print('Number of classes {}'.format(n_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading results for all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for algo in algo_arr:\n",
    "    all_dfs = []\n",
    "    for seed in range(1, 11):\n",
    "        file_name = path + dataset_name + '_' + algo + '_' + str(seed) + '.csv'\n",
    "        df = pd.read_csv(file_name)\n",
    "        df['fold'] = seed\n",
    "        all_dfs.append(df)\n",
    "        pass\n",
    "    \n",
    "    df = pd.concat(all_dfs)\n",
    "    df.name = algo\n",
    "    results[algo] = df\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution between classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.unique(data['target'], return_counts=True)\n",
    "print(y)\n",
    "plt.bar(x, y)\n",
    "plt.title('Distribution between classes for {} dataset'.format(dataset_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization for synthetic datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name.startswith('gen'):\n",
    "    col_arr = ['r', 'g', 'y', 'b']\n",
    "    for i in range(0,4):\n",
    "        # plot every 5th point to see something\n",
    "        plt.scatter(data['data'].T[0][i*2000: (i+1)*2000][::5], \n",
    "                    data['data'].T[1][i*2000: (i+1)*2000][::5], s=1,\n",
    "                    c=col_arr[i]\n",
    "                    )\n",
    "    plt.title('$\\sigma=1.0$')\n",
    "    plt.grid(True)\n",
    "    #tikzplotlib.save('../paper/sample-papers/plots/{}-class-visualization.tex'.format(\n",
    "    #    dataset_name.replace('.', '').replace('_', '-')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eps analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes there is small deviation from $\\epsilon$. All cases even with smallest differences are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_algos = []\n",
    "\n",
    "func_arr = ['marg_err', 'inv_err', 'inv_m_err']\n",
    "\n",
    "for algo in algo_arr:\n",
    "    print(algo)\n",
    "    df = results[algo]\n",
    "    #print(df.name)\n",
    "    df = df[['eps', 'marg_err', 'inv_err', 'inv_m_err']].groupby(['eps']).mean().copy()\n",
    "    #df.to_csv(path + 'A_significance_{}.csv'.format(dataset_name))\n",
    "    df['algo'] = algo\n",
    "    df.reset_index(inplace=True)\n",
    "    for func_name in func_arr:\n",
    "        if sum(df[func_name] <= df['eps']) < 6:\n",
    "            print('\\tNot calibrated {} for eps = {}'.format(func_name,\n",
    "                                                           df[df[func_name] > df['eps']]['eps'].values))\n",
    "    all_algos.append(df)\n",
    "    pass\n",
    "df = pd.concat(all_algos)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df.to_csv(path_sign + 'A_significance_{}.csv'.format(dataset_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_baseline_err(algo_arr, results, eps_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many cases deviate by threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res_df = []\n",
    "for algo in algo_arr:\n",
    "    df = results[algo].groupby('eps').mean()\n",
    "    df.reset_index(inplace=True)\n",
    "    df.drop(df[df['eps'] == 0.03].index, inplace=True)\n",
    "    df['algo'] = algo\n",
    "    all_res_df.append(df) \n",
    "all_res_df = pd.concat(all_res_df)\n",
    "all_res_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "cols = ['marg_oneC', 'inv_oneC', 'inv_m_oneC']\n",
    "all_res_df['diff_oneC'] = all_res_df[cols].max(axis=1) - all_res_df[cols].min(axis=1)\n",
    "\n",
    "cols = ['marg_avgC', 'inv_avgC', 'inv_m_avgC']\n",
    "all_res_df['diff_avgC'] = (all_res_df[cols].max(axis=1) - all_res_df[cols].min(axis=1)) / n_classes\n",
    "\n",
    "all_res_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res_df[['diff_oneC', 'diff_avgC']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a threshold here (fraction of 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneC_above = (all_res_df['diff_oneC'] > threshold).sum() / len(all_res_df) * 100\n",
    "print('for oneC {}% of cases differ at least by {}%'.format(oneC_above, threshold * 100))\n",
    "#oneC_above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgC_above = (all_res_df['diff_avgC'] > threshold).sum() / len(all_res_df) * 100\n",
    "print('for avgC {}% of cases differ at least by {}%'.format(avgC_above, threshold * 100))\n",
    "#avgC_above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# oneC vs E_oneC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res_df['eff_oneC_coef_m'] = all_res_df['marg_eff_oneC'] / all_res_df['marg_oneC']\n",
    "all_res_df['eff_oneC_coef_ip_m'] = all_res_df['inv_m_eff_oneC'] / all_res_df['inv_m_oneC']\n",
    "all_res_df['eff_oneC_coef_ip'] = all_res_df['inv_eff_oneC'] / all_res_df['inv_oneC']\n",
    "#all_res_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_oneC_df = all_res_df[['algo', 'eff_oneC_coef_m', 'eff_oneC_coef_ip_m', 'eff_oneC_coef_ip']].groupby('algo').mean()\n",
    "#eff_oneC_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_oneC_df.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_oneC_df.std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions\n",
    "def get_baseline_accuracy(algo_arr, results, eps_err):\n",
    "    baseline_err_arr = []\n",
    "    for algo in algo_arr:\n",
    "        df = results[algo]\n",
    "        baseline_err_arr.append(1 - df[df['eps'] == eps_err[0]]['origin_err'].mean())\n",
    "\n",
    "    return pd.DataFrame({'algo': algo_arr, 'b_err': baseline_err_arr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_acc = get_baseline_accuracy(algo_arr, results, eps_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = pd.DataFrame({\n",
    "    'acc' : baseline_acc.sort_values('algo')['b_err'].values,\n",
    "    'oneC' : eff_oneC_df.mean(axis=1).values\n",
    "})\n",
    "\n",
    "eff_oneC_corr = np.round(corr_df.corr().values[0,1], 3)\n",
    "print('Correlation between baseline accuracy and oneC = {}'.format(eff_oneC_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df.plot(\n",
    "    kind='line'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of oneC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_res_dic = {}\n",
    "for algo in algo_arr:\n",
    "    sub_dic = {}\n",
    "    stat_res_dic[algo] = {'oneC':{}, 'avgC':{}, 'eff_oneC':{}}\n",
    "#stat_res_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'oneC'\n",
    "col_str = '_{}'.format(col)\n",
    "plot_metric(results, col_str, nc_func_arr, algo_arr, eps_err, colours, ls_arr, lw_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting into files for paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# find max and min for plotting\n",
    "min_val = np.inf\n",
    "max_val = 0\n",
    "for algo in algo_arr:\n",
    "    new_min = np.min(np.min(results[algo][['eps', 'marg{}'.format(col_str),\n",
    "                                           'inv{}'.format(col_str),\n",
    "                                           'inv_m{}'.format(col_str)]].groupby('eps').mean()))\n",
    "    new_max = np.max(np.max(results[algo][['eps', 'marg{}'.format(col_str),\n",
    "                                           'inv{}'.format(col_str),\n",
    "                                           'inv_m{}'.format(col_str)]].groupby('eps').mean()))\n",
    "    if new_min < min_val:\n",
    "        min_val = new_min\n",
    "    if new_max > max_val:\n",
    "        max_val = new_max\n",
    "min_val = min(min_val, -0.05)\n",
    "print('{} - {}'.format(min_val, max_val))\n",
    "\n",
    "\n",
    "lw_arr_2 = [6, 6, 1]\n",
    "\n",
    "for k in [0, 2, 4, 6]:\n",
    "    plt.clf()\n",
    "    for i in [k, k+1]:\n",
    "        algo = algo_arr[i]\n",
    "        metric_df = results[algo][['eps', 'marg{}'.format(col_str),\n",
    "                                   'inv{}'.format(col_str),\n",
    "                                   'inv_m{}'.format(col_str)]].groupby('eps').mean()\n",
    "\n",
    "        for nc_idx in range(0, len(nc_func_arr)):\n",
    "            # print(nc_idx)\n",
    "            if nc_idx == 2:\n",
    "                label_str = algo\n",
    "            else:\n",
    "                label_str = '_nolegend_'\n",
    "            plt.plot(eps_err, metric_df[nc_func_arr[nc_idx] + col_str],\n",
    "                     c=colours[i], ls=ls_arr[nc_idx], lw=lw_arr[nc_idx],\n",
    "                     label=label_str\n",
    "                     )\n",
    "        pass\n",
    "    plt.xticks(eps_err)\n",
    "    plt.ylim((min_val * 0.9, max_val * 1.1))\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    tikzplotlib.save('../paper/sample-papers/plots/{}-oneC-{}.tex'.format(dataset_name,\n",
    "                                                                         k))\n",
    "    pass\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## oneC significance of differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = algo_arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_res = mean_test_for_algo(algo, results, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_res_threshold = mean_threshold_test_for_algo(algo, results, col,  n_classes, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_res = stats_test_for_algo(algo, results, col, p_val=p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_res_dic[algo][col]= (copy.deepcopy(mean_res), \n",
    "                          copy.deepcopy(mean_res_threshold), \n",
    "                          copy.deepcopy(stat_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = algo_arr[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_res = mean_test_for_algo(algo, results, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_res_threshold = mean_threshold_test_for_algo(algo, results, col,  n_classes, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stat_res = stats_test_for_algo(algo, results, col, p_val=p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_res_dic[algo][col]= (copy.deepcopy(mean_res), \n",
    "                          copy.deepcopy(mean_res_threshold), \n",
    "                          copy.deepcopy(stat_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = algo_arr[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_res = mean_test_for_algo(algo, results, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_res_threshold = mean_threshold_test_for_algo(algo_arr[2], results, col,  n_classes, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stat_res = stats_test_for_algo(algo_arr[2], results, col, p_val=p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_res_dic[algo][col]= (copy.deepcopy(mean_res), \n",
    "                          copy.deepcopy(mean_res_threshold), \n",
    "                          copy.deepcopy(stat_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = algo_arr[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_res = mean_test_for_algo(algo_arr[3], results, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_res_threshold = mean_threshold_test_for_algo(algo_arr[3], results, col,  n_classes, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stat_res = stats_test_for_algo(algo_arr[3], results, col, p_val=p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_res_dic[algo][col]= (copy.deepcopy(mean_res), \n",
    "                          copy.deepcopy(mean_res_threshold), \n",
    "                          copy.deepcopy(stat_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = algo_arr[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mean_res = mean_test_for_algo(algo_arr[4], results, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_res_threshold = mean_threshold_test_for_algo(algo_arr[4], results, col,  n_classes, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stat_res = stats_test_for_algo(algo_arr[4], results, col, p_val=p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_res_dic[algo][col]= (copy.deepcopy(mean_res), \n",
    "                          copy.deepcopy(mean_res_threshold), \n",
    "                          copy.deepcopy(stat_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = algo_arr[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_res = mean_test_for_algo(algo_arr[5], results, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_res_threshold = mean_threshold_test_for_algo(algo_arr[5], results, col,  n_classes, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stat_res = stats_test_for_algo(algo_arr[5], results, col, p_val=p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_res_dic[algo][col]= (copy.deepcopy(mean_res), \n",
    "                          copy.deepcopy(mean_res_threshold), \n",
    "                          copy.deepcopy(stat_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = algo_arr[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_res = mean_test_for_algo(algo_arr[6], results, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_res_threshold = mean_threshold_test_for_algo(algo_arr[6], results, col,  n_classes, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_res = stats_test_for_algo(algo_arr[6], results, col, p_val=p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_res_dic[algo][col]= (copy.deepcopy(mean_res), \n",
    "                          copy.deepcopy(mean_res_threshold), \n",
    "                          copy.deepcopy(stat_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = algo_arr[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_res = mean_test_for_algo(algo_arr[7], results, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_res_threshold = mean_threshold_test_for_algo(algo_arr[7], results, col,  n_classes, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stat_res = stats_test_for_algo(algo_arr[7], results, col, p_val=p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_res_dic[algo][col]= (copy.deepcopy(mean_res), \n",
    "                          copy.deepcopy(mean_res_threshold), \n",
    "                          copy.deepcopy(stat_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eff_oneC\n",
    "$eff\\_oneC = \\frac{number\\ \\ of\\ \\ correst\\ \\ singleton\\ \\ predictions}{total\\ \\ number\\ \\ of\\ \\ intsnaces}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "col = 'eff_oneC'\n",
    "col_str = '_{}'.format(col)\n",
    "plot_metric(results, col_str, nc_func_arr, algo_arr, eps_err, colours, ls_arr, lw_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eff_oneC stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_res = stats_test_for_algo(algo_arr[0], results, col, p_val=p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stat_res = stats_test_for_algo(algo_arr[1], results, col, p_val=p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stat_res = stats_test_for_algo(algo_arr[2], results, col, p_val=p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_res = stats_test_for_algo(algo_arr[3], results, col, p_val=p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stat_res = stats_test_for_algo(algo_arr[4], results, col, p_val=p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stat_res = stats_test_for_algo(algo_arr[5], results, col, p_val=p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_res = stats_test_for_algo(algo_arr[6], results, col, p_val=p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stat_res = stats_test_for_algo(algo_arr[7], results, col, p_val=p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# avg_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'avgC'\n",
    "col_str = '_{}'.format(col)\n",
    "plot_metric(results, col_str, nc_func_arr, algo_arr, eps_err, colours, ls_arr, lw_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting into files for paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# find max and min for plottin\n",
    "min_val = np.inf\n",
    "max_val = 0\n",
    "for algo in algo_arr:\n",
    "    new_min = np.min(np.min(results[algo][['eps', 'marg{}'.format(col_str),\n",
    "                                           'inv{}'.format(col_str),\n",
    "                                           'inv_m{}'.format(col_str)]].groupby('eps').mean()))\n",
    "    new_max = np.max(np.max(results[algo][['eps', 'marg{}'.format(col_str),\n",
    "                                           'inv{}'.format(col_str),\n",
    "                                           'inv_m{}'.format(col_str)]].groupby('eps').mean()))\n",
    "    if new_min < min_val:\n",
    "        min_val = new_min\n",
    "    if new_max > max_val:\n",
    "        max_val = new_max\n",
    "min_val = min(min_val, -0.05)\n",
    "print('{} - {}'.format(min_val, max_val))\n",
    "\n",
    "for k in [0, 2, 4, 6]:\n",
    "    plt.clf()\n",
    "    for i in [k, k+1]:\n",
    "        algo = algo_arr[i]\n",
    "        metric_df = results[algo][['eps', 'marg{}'.format(col_str),\n",
    "                                   'inv{}'.format(col_str),\n",
    "                                   'inv_m{}'.format(col_str)]].groupby('eps').mean()\n",
    "\n",
    "        for nc_idx in range(0, len(nc_func_arr)):\n",
    "            # print(nc_idx)\n",
    "            if nc_idx == 2:\n",
    "                label_str = algo\n",
    "            else:\n",
    "                label_str = '_nolegend_'\n",
    "            plt.plot(eps_err, metric_df[nc_func_arr[nc_idx] + col_str],\n",
    "                     c=colours[i], ls=ls_arr[nc_idx], lw=lw_arr[nc_idx],\n",
    "                     label=label_str\n",
    "                     )\n",
    "        pass\n",
    "    plt.xticks(eps_err)\n",
    "    plt.ylim((min_val * 0.9, max_val * 1.1))\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    tikzplotlib.save('../paper/sample-papers/plots/{}-{}-{}.tex'.format(dataset_name,\n",
    "                                                                        col,\n",
    "                                                                         k))\n",
    "    pass\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## avgC stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = algo_arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mean_res = mean_test_for_algo(algo_arr[0], results, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_res_threshold = mean_threshold_test_for_algo(algo_arr[0], results, col,  n_classes, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_res = stats_test_for_algo(algo_arr[0], results, col, p_val=p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_res_dic[algo][col]= (copy.deepcopy(mean_res), \n",
    "                          copy.deepcopy(mean_res_threshold), \n",
    "                          copy.deepcopy(stat_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = algo_arr[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_res = mean_test_for_algo(algo_arr[1], results, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_res_threshold = mean_threshold_test_for_algo(algo_arr[1], results, col,  n_classes, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stat_res = stats_test_for_algo(algo_arr[1], results, col, p_val=p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_res_dic[algo][col]= (copy.deepcopy(mean_res), \n",
    "                          copy.deepcopy(mean_res_threshold), \n",
    "                          copy.deepcopy(stat_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = algo_arr[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_res = mean_test_for_algo(algo_arr[2], results, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_res_threshold = mean_threshold_test_for_algo(algo_arr[2], results, col,  n_classes, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stat_res = stats_test_for_algo(algo_arr[2], results, col, p_val=p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_res_dic[algo][col]= (copy.deepcopy(mean_res), \n",
    "                          copy.deepcopy(mean_res_threshold), \n",
    "                          copy.deepcopy(stat_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = algo_arr[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_res = mean_test_for_algo(algo_arr[3], results, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_res_threshold = mean_threshold_test_for_algo(algo_arr[3], results, col,  n_classes, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stat_res = stat_res = stats_test_for_algo(algo_arr[3], results, col, p_val=p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_res_dic[algo][col]= (copy.deepcopy(mean_res), \n",
    "                          copy.deepcopy(mean_res_threshold), \n",
    "                          copy.deepcopy(stat_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = algo_arr[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_res = mean_test_for_algo(algo_arr[4], results, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_res_threshold = mean_threshold_test_for_algo(algo_arr[4], results, col,  n_classes, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stat_res = stats_test_for_algo(algo_arr[4], results, col, p_val=p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_res_dic[algo][col]= (copy.deepcopy(mean_res), \n",
    "                          copy.deepcopy(mean_res_threshold), \n",
    "                          copy.deepcopy(stat_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = algo_arr[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_res = mean_test_for_algo(algo_arr[5], results, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_res_threshold = mean_threshold_test_for_algo(algo_arr[5], results, col,  n_classes, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stat_res = stats_test_for_algo(algo_arr[5], results, col, p_val=p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_res_dic[algo][col]= (copy.deepcopy(mean_res), \n",
    "                          copy.deepcopy(mean_res_threshold), \n",
    "                          copy.deepcopy(stat_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = algo_arr[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_res = mean_test_for_algo(algo_arr[6], results, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_res_threshold = mean_threshold_test_for_algo(algo_arr[6], results, col,  n_classes, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_res = stats_test_for_algo(algo_arr[6], results, col, p_val=p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_res_dic[algo][col]= (copy.deepcopy(mean_res), \n",
    "                          copy.deepcopy(mean_res_threshold), \n",
    "                          copy.deepcopy(stat_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = algo_arr[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_res = mean_test_for_algo(algo_arr[7], results, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_res_threshold = mean_threshold_test_for_algo(algo_arr[7], results, col,  n_classes, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stat_res = stats_test_for_algo(algo_arr[7], results, col, p_val=p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_res_dic[algo][col]= (copy.deepcopy(mean_res), \n",
    "                          copy.deepcopy(mean_res_threshold), \n",
    "                          copy.deepcopy(stat_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_res_val = 0\n",
    "mean_res_th_val = 1\n",
    "stat_res_val = 2\n",
    "\n",
    "func_short_arr = ['IP', 'IP_M', 'M']\n",
    "ip = 'IP'\n",
    "ip_m = 'IP_M'\n",
    "m = 'M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_pattern = {}\n",
    "main_pattern_oneC = {}\n",
    "main_pattern_avgC = {}\n",
    "inverse_pattern = {}\n",
    "margin_best = {}\n",
    "ip_best = {}\n",
    "ip_m_best = {}\n",
    "ip_not_expected = {}\n",
    "margin_better_ip = {}\n",
    "patterns_dic = {\n",
    "    'is_main_pattern': main_pattern,\n",
    "    'is_main_pattern_oneC': main_pattern_oneC,\n",
    "    'is_main_pattern_avgC': main_pattern_avgC,\n",
    "    'is_inverse_pattern': inverse_pattern,\n",
    "    'is_margin_best': margin_best,\n",
    "    'is_ip_best': ip_best,\n",
    "    'is_ip_m_best': ip_m_best,\n",
    "    'is_ip_not_expected': ip_not_expected,\n",
    "    'is_margin_better_ip': margin_better_ip,\n",
    "}\n",
    "\n",
    "\n",
    "def add_val(patterns_dic, key, eps, algo):\n",
    "    if algo in patterns_dic[key]:\n",
    "        patterns_dic[key][algo].append(eps)\n",
    "    else:\n",
    "        patterns_dic[key][algo] = [eps]\n",
    "    pass\n",
    "\n",
    "\n",
    "for algo in algo_arr:\n",
    "    for eps in eps_err:\n",
    "        if eps == 0.03:\n",
    "            continue\n",
    "        # condence oneC\n",
    "        oneC_res_th_df = stat_res_dic[algo]['oneC'][mean_res_th_val][str(eps)]\n",
    "        oneC_stat_df = stat_res_dic[algo]['oneC'][stat_res_val][str(eps)]\n",
    "        oneC_df = pd.DataFrame({})\n",
    "        oneC_df['n-func'] = func_short_arr\n",
    "        for func_short in func_short_arr:\n",
    "            th_arr = oneC_res_th_df[func_short].values\n",
    "            st_arr = oneC_stat_df[func_short].values\n",
    "            final_arr = []\n",
    "            for i in range(0, 3):\n",
    "                if np.isnan(th_arr[i]):\n",
    "                    final_arr.append(st_arr[i])\n",
    "                else:\n",
    "                    final_arr.append(th_arr[i])\n",
    "            oneC_df[func_short] = final_arr\n",
    "        oneC_df.set_index('n-func', inplace=True)\n",
    "\n",
    "        # condence avgC\n",
    "        avgC_res_th_df = stat_res_dic[algo]['avgC'][mean_res_th_val][str(eps)]\n",
    "        avgC_stat_df = stat_res_dic[algo]['avgC'][stat_res_val][str(eps)]\n",
    "\n",
    "        avgC_df = pd.DataFrame({})\n",
    "        avgC_df['n-func'] = func_short_arr\n",
    "        for func_short in func_short_arr:\n",
    "            th_arr = avgC_res_th_df[func_short].values\n",
    "            st_arr = avgC_stat_df[func_short].values\n",
    "            final_arr = []\n",
    "            for i in range(0, 3):\n",
    "                if np.isnan(th_arr[i]):\n",
    "                    final_arr.append(st_arr[i])\n",
    "                else:\n",
    "                    final_arr.append(th_arr[i])\n",
    "            avgC_df[func_short] = final_arr\n",
    "        avgC_df.set_index('n-func', inplace=True)\n",
    "\n",
    "        is_main_pattern = ((oneC_df.T[m][ip] == 1) and (avgC_df.T[ip][m] == 1))\n",
    "        if is_main_pattern:\n",
    "            key = 'is_main_pattern'\n",
    "            add_val(patterns_dic, key, eps, algo)\n",
    "\n",
    "        is_main_pattern_oneC = (oneC_df.T[m][ip] == 1) and np.isnan(avgC_df.T[ip][m])\n",
    "        if is_main_pattern_oneC:\n",
    "            key = 'is_main_pattern_oneC'\n",
    "            add_val(patterns_dic, key, eps, algo)\n",
    "\n",
    "        is_main_pattern_avgC = np.isnan(oneC_df.T[m][ip]) and (avgC_df.T[ip][m] == 1)\n",
    "        if is_main_pattern_avgC:\n",
    "            key = 'is_main_pattern_avgC'\n",
    "            add_val(patterns_dic, key, eps, algo)\n",
    "\n",
    "        is_inverse_pattern = (oneC_df.T[ip][m] == 1) and (avgC_df.T[m][ip] == 1)\n",
    "        if is_inverse_pattern:\n",
    "            key = 'is_inverse_pattern'\n",
    "            add_val(patterns_dic, key, eps, algo)\n",
    "\n",
    "        is_margin_best = (~np.isnan(oneC_df.T[m][ip]) or ~np.isnan(oneC_df.T[m][ip_m]) or ~np.isnan(\n",
    "            avgC_df.T[m][ip]) or ~np.isnan(avgC_df.T[m][ip_m])) and (\n",
    "                                 ((oneC_df.T[m][ip] == 1) or np.isnan(oneC_df.T[m][ip])) and (\n",
    "                                 (oneC_df.T[m][ip_m] == 1) or np.isnan(oneC_df.T[m][ip_m])) and (\n",
    "                                         (avgC_df.T[m][ip] == 1) or (avgC_df.T[m][ip])) and (\n",
    "                                         (avgC_df.T[m][ip_m] == 1) or np.isnan(avgC_df.T[m][ip_m])))\n",
    "        if is_margin_best:\n",
    "            key = 'is_margin_best'\n",
    "            add_val(patterns_dic, key, eps, algo)\n",
    "\n",
    "        is_ip_best = (~np.isnan(oneC_df.T[ip][m]) or ~np.isnan(oneC_df.T[ip][ip_m]) or ~np.isnan(\n",
    "            avgC_df.T[ip][m]) or ~np.isnan(avgC_df.T[ip][ip_m])) and (\n",
    "                             ((oneC_df.T[ip][m] == 1) or np.isnan(oneC_df.T[ip][m])) and (\n",
    "                             (oneC_df.T[ip][ip_m] == 1) or np.isnan(oneC_df.T[ip][ip_m])) and (\n",
    "                                     (avgC_df.T[ip][m] == 1) or np.isnan(avgC_df.T[ip][m])) and (\n",
    "                                     (avgC_df.T[ip][ip_m] == 1) or np.isnan(avgC_df.T[ip][ip_m])))\n",
    "        if is_ip_best:\n",
    "            key = 'is_ip_best'\n",
    "            add_val(patterns_dic, key, eps, algo)\n",
    "\n",
    "        is_margin_better_ip = ((oneC_df.T[m][ip] == 1) and (avgC_df.T[m][ip] == 1)) or (\n",
    "                (oneC_df.T[m][ip] == 1) and np.isnan(avgC_df.T[m][ip])) or (\n",
    "                                      np.isnan(oneC_df.T[m][ip]) and (avgC_df.T[m][ip] == 1))\n",
    "        if is_margin_better_ip:\n",
    "            key = 'is_margin_better_ip'\n",
    "            add_val(patterns_dic, key, eps, algo)\n",
    "        \"\"\"\n",
    "        is_ip_m_best = (~np.isnan(oneC_df.T[ip_m][m]) or ~np.isnan(oneC_df.T[ip_m][ip]) or ~np.isnan(\n",
    "            avgC_df.T[ip_m][m]) or ~np.isnan(avgC_df.T[ip_m][ip])) and (\n",
    "                               ((oneC_df.T[ip_m][m] == 1) or np.isnan(oneC_df.T[ip_m][m])) and (\n",
    "                               (oneC_df.T[ip_m][ip] == 1) or np.isnan(oneC_df.T[ip_m][ip])) and (\n",
    "                                       (avgC_df.T[ip_m][m] == 1) or np.isnan(avgC_df.T[ip_m][m])) and (\n",
    "                                       (avgC_df.T[ip_m][ip] == 1) or np.isnan(avgC_df.T[ip_m][ip])))\n",
    "        \"\"\"\n",
    "\n",
    "        is_ip_m_best = (oneC_df.T[ip_m][m] == 1) and (oneC_df.T[ip_m][ip] == 1) and (avgC_df.T[ip_m][m] == 1) and (\n",
    "                    avgC_df.T[ip_m][ip] == 1)\n",
    "        if is_ip_m_best:\n",
    "            key = 'is_ip_m_best'\n",
    "            add_val(patterns_dic, key, eps, algo)\n",
    "\n",
    "        is_ip_not_expected = is_main_pattern and (oneC_df.T[ip][ip_m] == 1) or (avgC_df.T[ip][ip_m] == 1)\n",
    "        if is_ip_not_expected:\n",
    "            key = 'is_ip_not_expected'\n",
    "            add_val(patterns_dic, key, eps, algo)\n",
    "        pass\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    \n",
    "    for key in patterns_dic.keys():\n",
    "        if len(patterns_dic[key]) > 0:\n",
    "            str_ = '{}\\n\\t{}'.format(key, patterns_dic[key])\n",
    "            print(str_)\n",
    "            #f.write(str_ + '\\n')\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing patterns to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "with open('analysis-results/patterns/{}'.format(dataset_name), 'a') as f:\n",
    "\n",
    "    for key in patterns_dic.keys():\n",
    "        if len(patterns_dic[key]) > 0:\n",
    "            str_ = '{}\\n\\t{}'.format(key, patterns_dic[key])\n",
    "            print(str_)\n",
    "            f.write(str_ + '\\n')\n",
    "        pass\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
